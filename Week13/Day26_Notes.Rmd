---
title: "Day 36 Notes: Linear Regression III"
output:
  html_document:
    css: ./style.css
    toc: yes
    toc_float: yes
    theme: cosmo
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r global_options, include = FALSE}
library(knitr)
library(tidyverse)
library(palmerpenguins)
knitr::opts_chunk$set(eval = TRUE, results = TRUE)
```

---

## Linear Regression

Suppose we consider a model 

$$\hat{Y_i} = b_1 X_i + b_0$$

Our hope is that our estimations are pretty close to the actual values.
That is, we hope that:

$$Y_i = b_1 X_i + b_0 + \epsilon$$


$H_0: Y_i = b_0 + \epsilon$: the null model is better at predicting $Y_i$ 
$H_A: Y_i = b_1 X_i + b_0 + \epsilon$: the alternate model is better at predicting $Y_i$



```{r}
load("~/MAT104-Fall2024/Week12/parenthood.Rdata")
dansleep <- parenthood$dan.sleep
babysleep <- parenthood$baby.sleep
grump <- parenthood$dan.grump
set.seed(33)
GPA <- rnorm(100,3,.3)
```

$H_0:$ Danielle's grumpiness is due to random variation
$H_A:$ Danielles's sleep predicts her grumpiness better than random variation.


```{r}
# To find the line of best fit we use the lm() function:
summary(lm(grump ~ dansleep))
```


## Random Variation


```{r}
summary(lm(parenthood$dan.grump~GPA))
```



```{r}
df<-data.frame(grump,GPA)
ggplot(df,aes(x=GPA,y=grump))+geom_point() + geom_abline(intercept = 59.117, slope = 1.522)
```


### Pitfalls

+ Just because a linear model performs better than random variation does not mean the model performs well. Compare the following two linear regressions:


```{r}
summary(lm(grump ~ dansleep))
summary(lm(grump ~ babysleep))
```



### Multiple Regression Model



```{r}
summary(lm(grump ~ dansleep + babysleep))
```



## Confidence Intervals for the Coefficients


From this we can say:

```{r}
-8.9368 - qt(.975,97)*.4285
-8.9368 + qt(.975,97)*.4285
# We are 95% confident that the coefficient for Danielle's sleep is between -9.79 and -8.09
```

